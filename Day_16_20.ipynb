{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdzKMJ/GpV1KVC0fi5BG4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaidosa/100-days-of-DS/blob/main/Day_16_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 16- Decision Trees\n",
        "\n",
        "Today we'll look into the one of the popular machine learning algorithm used for both classification and regression tasks.<br/>\n",
        "The algorithm builds a tree-like model of decisions and their possible consequences. Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a  class label or a predicted value. <br/>\n",
        "\n",
        "The key idea behind Decision Trees is to select the best attribute at each node that splits the data in a way that maximize the separation of the classes or miniminzes the impurity within each split. This process is repeated recursively until a stopping criterion is met, such as reaching a maximum depth or having a minimum number of samples in a leaf node. <br/>\n",
        "The two main types of Decision Trees are:<br/>\n",
        "  1. Classification Tress: Used for classifying data into discrete classes or categories.\n",
        "  2. Regression Trees: Used for predicting continous values.\n",
        "\n",
        "Decision Trees have several advantages:<br/>\n",
        "  * They are easy to understand and interpret.\n",
        "  * They can handle both categorical and numerical features.\n",
        "  * They can capture complex non-linear relationships in the data.\n",
        "  * They can handle missing values by using surrogate splits.\n",
        "\n",
        "However, Decision Trees also have some limitations:<br/>\n",
        "  * They can easily overfit the training data, resulting in poor generalization to unseen data.\n",
        "  * They are sensitive to small changes in the training data, which can lead to different tree structures.\n",
        "  * They can be biased towards attributes with more levels or values.\n",
        "  * They may not perform well on imbalance datasets. \n",
        "\n",
        "That's all for today, I encourage you to go through the codes and read more over the internet if required. In the next day we'll go through the Random forest succesor of decision tree algorithms."
      ],
      "metadata": {
        "id": "lZtU4xq1hW-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpI7_2xHhKFU"
      },
      "outputs": [],
      "source": []
    }
  ]
}